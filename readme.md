# This is the instruction for UTBoost

Here is the structure of the artifact:
1. dir_generated_test_cases are the test cases generated by UTGenerator
2. update-SWE-Bench contains the file and data we need to update SWE-Bench
3. UTGenerator contains the code for tes tcase generation
4. log_intra is the intramorphic tesing algorithm to report whether the gold patch and the generated patch behave the same
5. augTest.json contains the augmented test cases we confirm in the paper. You can use it and the improved parser to reproduce the results through SWE-Bench evaluation.

## Generating test cases
first we need to create an environment with python 3.11, and install
```
openai
pre-commit
datasets
tiktoken
swebench
libcst
```
Then we should first locate the places for adding test cases by:
```
python augtest/fl/localize.py --file_level --related_level --fine_grain_line_level \
                                --output_folder results_224/location --top_n 3 \
                                --compress \
                                --context_window=10 \
                                --temperature 0.8 \
                                --num_samples 4
python augtest/fl/localize.py --merge \
                                --output_folder results_224/location_merged \
                                --start_file results_224/location/loc_outputs.jsonl \
                                --num_samples 4
```
Then we can run the test case generation script:
```
python augtest/repair/repair.py --loc_file results_224/location_merged/loc_merged_0-1_outputs.jsonl \
                                  --output_folder results_224/new_gen_testCase_t099_lm01 \
                                  --loc_interval --top_n=3 --context_window=10 \
                                  --max_samples 2  --cot --diff_format \
                                  --gen_and_process 
```

## SWE-Bench evaluation with augmentest test cases
1. WE should replace the original test cases with the generated test cases, and update the annoation data. The data are in update-SWE-Bench.
2. Then we need to run the SWE-Bench and get the log report of every methods.
3. We can compare the new annotation `update-SWE-Bench/perfect_parser_test_instance_dict.json` with the one in SWE-Bench's huaggingface data.

## Report suspicious issues with intramorphic testing
1. We need to run `log_intra.py` to get the report of suspicious issues.
2. Then we can check if UTGenerator generate effective and harmless code, and then add them to SWE-Bench after confirmation.
